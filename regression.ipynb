{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 로드 (사용자 데이터 구조 예시)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# 아래는 설명을 위한 가상 데이터 생성 예시입니다.\n",
    "df = pd.read_csv(\"./final/data/refined/LGES_final.csv\")\n",
    "df = df.sort_values('Date')\n",
    "df = df.fillna(method='ffill')\n",
    "df = df.rename(columns={'y(stock)': 'y'})\n",
    "\n",
    "# --- [1] 데이터 전처리: 날짜별 Aggregation ---\n",
    "# E1~E7은 합산 후 x1으로 나누어 비율로 변환\n",
    "e_columns = ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7']\n",
    "macro_columns = ['usd_krw', 'vix'] # 예시 지표\n",
    "\n",
    "# 날짜별 그룹화\n",
    "agg_dict = {col: 'sum' for col in e_columns}\n",
    "agg_dict.update({col: 'first' for col in ['x1', 'y'] + macro_columns})\n",
    "df_daily = df.groupby('Date').agg(agg_dict).reset_index()\n",
    "\n",
    "# 주제별 노출도 계산 (E_i / x1)\n",
    "for col in e_columns:\n",
    "    df_daily[col] = df_daily[col] / df_daily['x1']\n",
    "\n",
    "# 타겟 변수 절대값 변환 및 x1 로그 변환\n",
    "df_daily['log_x1'] = np.log1p(df_daily['x1']) # log(1+x)로 안정성 확보\n",
    "\n",
    "# 분석에 사용할 최종 피쳐 설정\n",
    "features = e_columns + ['log_x1'] + macro_columns\n",
    "X = df_daily[features]\n",
    "y = df_daily['y']\n",
    "\n",
    "# --- [2] 다중공선성(VIF) 확인 ---\n",
    "# 스케일링 전 VIF 계산을 위해 상수항 추가\n",
    "# VIF 계산용 데이터프레임 준비\n",
    "X_vif = X.copy()\n",
    "X_vif = sm.add_constant(X_vif) \n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(X_vif.columns))]\n",
    "print(\"--- [VIF 결과 (10 이상 주의)] ---\")\n",
    "print(vif_data)\n",
    "\n",
    "# --- [3] 통계적 유의성 확인 (OLS Regression) ---\n",
    "# 계수 비교를 위해 독립변수 표준화(Standardization) 수행\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_scaled = sm.add_constant(X_scaled) # 상수항 추가\n",
    "\n",
    "model = sm.OLS(y.values, X_scaled).fit()\n",
    "print(\"\\n--- [회귀 분석 요약 결과 (P>|t| 확인)] ---\")\n",
    "print(model.summary())\n",
    "\n",
    "# --- [4] 피쳐 중요도 산출 및 기여도 해석 ---\n",
    "# 표준화된 계수(Standardized Coefficients)를 중요도로 사용\n",
    "importance = model.params.drop('const').abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- [피쳐 중요도 (절대적 영향력 순위)] ---\")\n",
    "print(importance)\n",
    "\n",
    "# 예시 출력: 특정 날짜의 기여도 분석\n",
    "sample_date_idx = 0\n",
    "contribution = (X_scaled.iloc[sample_date_idx] * model.params).drop('const')\n",
    "print(f\"\\n[{df_daily.iloc[sample_date_idx]['Date']}] 실제 변동폭: {y.iloc[sample_date_idx]:.2f}%\")\n",
    "print(\"요인별 기여도:\")\n",
    "print(contribution.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c4325",
   "metadata": {},
   "source": [
    "### Logistic Reg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a576cae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>x1</th>\n",
       "      <th>E1</th>\n",
       "      <th>usd_krw</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-10</td>\n",
       "      <td>42</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>1432.60</td>\n",
       "      <td>-0.645161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-11</td>\n",
       "      <td>25</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1428.50</td>\n",
       "      <td>-1.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>43</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>1429.47</td>\n",
       "      <td>7.010582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-13</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1434.82</td>\n",
       "      <td>-1.112485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-16</td>\n",
       "      <td>28</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>1437.86</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  x1        E1  usd_krw         y\n",
       "0  2024-12-10  42  0.047619  1432.60 -0.645161\n",
       "1  2024-12-11  25  0.200000  1428.50 -1.818182\n",
       "2  2024-12-12  43  0.069767  1429.47  7.010582\n",
       "3  2024-12-13  29  0.000000  1434.82 -1.112485\n",
       "4  2024-12-16  28  0.071429  1437.86 -0.500000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daily[['Date', 'x1', 'E1', 'usd_krw', 'y']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051ca183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.679023\n",
      "         Iterations 5\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  y_bin   No. Observations:                  233\n",
      "Model:                          Logit   Df Residuals:                      222\n",
      "Method:                           MLE   Df Model:                           10\n",
      "Date:                Tue, 23 Dec 2025   Pseudo R-squ.:                 0.02036\n",
      "Time:                        22:42:19   Log-Likelihood:                -158.21\n",
      "converged:                       True   LL-Null:                       -161.50\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.7646\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0054      0.133      0.040      0.968      -0.255       0.266\n",
      "E1            -0.1959      0.143     -1.370      0.171      -0.476       0.084\n",
      "E2             0.0245      0.144      0.170      0.865      -0.257       0.306\n",
      "E3             0.0314      0.145      0.216      0.829      -0.253       0.316\n",
      "E4            -0.0576      0.140     -0.412      0.680      -0.332       0.217\n",
      "E5            -0.0785      0.154     -0.508      0.611      -0.381       0.224\n",
      "E6            -0.0128      0.135     -0.095      0.925      -0.278       0.253\n",
      "E7            -0.0464      0.138     -0.335      0.738      -0.318       0.225\n",
      "log_x1         0.2007      0.163      1.235      0.217      -0.118       0.519\n",
      "usd_krw       -0.0583      0.148     -0.393      0.694      -0.349       0.232\n",
      "vix           -0.2146      0.153     -1.404      0.160      -0.514       0.085\n",
      "==============================================================================\n",
      "\n",
      "--- [오즈비 분석 (1보다 클수록 상승 기여)] ---\n",
      "         Odds Ratio   P-value\n",
      "log_x1     1.222244  0.216891\n",
      "E3         1.031892  0.828709\n",
      "E2         1.024762  0.864904\n",
      "const      1.005365  0.967900\n",
      "E6         0.987274  0.924656\n",
      "E7         0.954694  0.737688\n",
      "E4         0.944013  0.680385\n",
      "usd_krw    0.943365  0.694085\n",
      "E5         0.924532  0.611323\n",
      "E1         0.822083  0.170771\n",
      "vix        0.806902  0.160297\n"
     ]
    }
   ],
   "source": [
    "df_daily = df_daily[df_daily['y'] != 0].copy()\n",
    "df_daily = df_daily.reset_index(drop=True) # 인덱스를 X와 맞추기 위해 초기화\n",
    "df_daily['y_bin'] = (df_daily['y'] > 0).astype(int)\n",
    "\n",
    "# --- [2] 데이터 준비 ---\n",
    "e_columns = ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7']\n",
    "macro_columns = ['usd_krw', 'vix']\n",
    "features = e_columns + ['log_x1'] + macro_columns\n",
    "\n",
    "X = df_daily[features]\n",
    "y = df_daily['y_bin']\n",
    "\n",
    "# --- [3] 스케일링 및 상수항 추가 ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# --- [4] 로지스틱 회귀 수행 (Logit) ---\n",
    "logit_model = sm.Logit(y, X_scaled).fit()\n",
    "\n",
    "print(logit_model.summary())\n",
    "\n",
    "# --- [5] 오즈비(Odds Ratio) 계산 ---\n",
    "# 로지스틱 회귀 계수를 지수함수(exp)로 변환하면 영향력을 직관적으로 이해하기 좋습니다.\n",
    "# 1보다 크면 상승 확률 증가, 1보다 작으면 상승 확률 감소를 의미합니다.\n",
    "odds_ratios = pd.DataFrame({\n",
    "    'Odds Ratio': np.exp(logit_model.params),\n",
    "    'P-value': logit_model.pvalues\n",
    "}).sort_values('Odds Ratio', ascending=False)\n",
    "\n",
    "print(\"\\n--- [오즈비 분석 (1보다 클수록 상승 기여)] ---\")\n",
    "print(odds_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb11bb3e",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd37de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "\n",
    "# --- [설정] 파일 목록 및 경로 ---\n",
    "base_path = \"./final/data/refined/\"\n",
    "output_path = \"./shap_results/\"\n",
    "\n",
    "# 결과 저장 폴더 생성\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "file_names = [\n",
    "    \"DoosanEnerbility_final\",\n",
    "    \"HanwhaAerospace_final\",\n",
    "    \"HDHyundaiHeavy_final\",\n",
    "    \"HyundaiMotor_final\",\n",
    "    \"KBFinancial_final\",\n",
    "    \"Kia_final\",\n",
    "    \"LGES_final\",\n",
    "    \"SamsungBio_final\",\n",
    "    \"SamsungElectronics_final\",\n",
    "    \"SKHynix_final\"\n",
    "]\n",
    "\n",
    "# --- [전처리 및 모델링 함수 정의] ---\n",
    "def process_stock_shap(file_name):\n",
    "    print(f\"Processing: {file_name}...\")\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    file_path = os.path.join(base_path, f\"{file_name}.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.sort_values('Date')\n",
    "    df = df.ffill().bfill() # 최신 pandas 문법 권장\n",
    "    df = df.rename(columns={'y(stock)': 'y'})\n",
    "\n",
    "    # 2. 데이터 전처리\n",
    "    e_columns = ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7']\n",
    "    # 매크로 지표가 파일에 있을 경우를 대비해 정의 (없으면 무시하도록 처리 가능하나, 기존 코드 로직 유지)\n",
    "    macro_columns = ['usd_krw', 'vix'] \n",
    "    \n",
    "    # agg_dict 생성 (컬럼이 실제 데이터에 있는지 확인 후 적용)\n",
    "    agg_dict = {col: 'sum' for col in e_columns}\n",
    "    existing_macros = [col for col in macro_columns if col in df.columns]\n",
    "    agg_dict.update({col: 'first' for col in existing_macros + ['x1', 'y']})\n",
    "\n",
    "    df_daily = df.groupby('Date').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # y=0 제거\n",
    "    df_daily = df_daily[df_daily['y'] != 0].copy()\n",
    "    df_daily = df_daily.reset_index(drop=True)\n",
    "\n",
    "    # 주제별 노출도 계산 (E_i / x1)\n",
    "    for col in e_columns:\n",
    "        df_daily[col] = df_daily[col] / df_daily['x1']\n",
    "\n",
    "    # 타겟 변수 및 피쳐 생성\n",
    "    df_daily['log_x1'] = np.log1p(df_daily['x1'])\n",
    "    df_daily['y_bin'] = (df_daily['y'] > 0).astype(int)\n",
    "\n",
    "    features = e_columns + ['log_x1']\n",
    "    X = df_daily[features]\n",
    "    y = df_daily['y_bin']\n",
    "\n",
    "    # 3. 모델링 (Random Forest)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "    # 전체 데이터셋에 대해 SHAP 값을 뽑기 위해 split은 학습용으로만 사용하고\n",
    "    # SHAP 계산은 X_scaled 전체를 대상으로 합니다.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # 4. SHAP Value 계산\n",
    "    explainer = shap.TreeExplainer(rf_model)\n",
    "    shap_values = explainer.shap_values(X_scaled)\n",
    "\n",
    "    # 차원 확인 및 추출 (Class 1: 상승에 대한 기여도 추출)\n",
    "    if isinstance(shap_values, list):\n",
    "        actual_shap_values = shap_values[1]\n",
    "    elif len(shap_values.shape) == 3:\n",
    "        actual_shap_values = shap_values[:, :, 1]\n",
    "    else:\n",
    "        actual_shap_values = shap_values\n",
    "\n",
    "    # 5. 결과 데이터프레임 생성 (Daily SHAP)\n",
    "    # 날짜 컬럼 추가\n",
    "    result_df = pd.DataFrame(actual_shap_values, columns=features)\n",
    "    result_df.insert(0, 'Date', df_daily['Date'])\n",
    "\n",
    "    # 6. Summary Row 생성 (Global Feature Importance)\n",
    "    # SHAP Summary Plot에서 보여주는 '변수의 중요도'는 절대값의 평균입니다.\n",
    "    summary_values = result_df[features].abs().mean()\n",
    "    \n",
    "    # Summary 행 생성 (Date에는 식별자 입력)\n",
    "    summary_row = pd.DataFrame(summary_values).T\n",
    "    summary_row.insert(0, 'Date', 'Global_Importance (Mean(|SHAP|))')\n",
    "    \n",
    "    # 최종 병합 (일별 데이터 + 마지막 줄 Summary)\n",
    "    final_output = pd.concat([result_df, summary_row], ignore_index=True)\n",
    "\n",
    "    #return final_output\n",
    "    # 7. CSV 저장\n",
    "    save_name = f\"SHAP_{file_name}.csv\"\n",
    "    save_path = os.path.join(output_path, save_name)\n",
    "    final_output.to_csv(save_path, index=False)\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "# --- [메인 실행 루프] ---\n",
    "if __name__ == \"__main__\":\n",
    "    for file_name in file_names:\n",
    "        try:\n",
    "            process_stock_shap(file_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {file_name}: {e}\")\n",
    "\n",
    "    print(\"\\nAll processes completed.\")\n",
    "\n",
    "# file = \"SamsungElectronics_final\"\n",
    "# final_df = process_stock_shap(file)\n",
    "# final_df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
